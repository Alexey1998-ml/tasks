{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recomendation_system.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1BaX2YjVBAqphzusXOKIixMYLiz5W7sGA",
      "authorship_tag": "ABX9TyMqREOxteLYKSx50DD3q8p2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alexey1998-ml/tasks/blob/master/recomendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2RyhEmGB80y",
        "outputId": "2370e46a-39f8-4de5-c1e6-1b5c894df773"
      },
      "source": [
        "!unzip drive/MyDrive/nlp/archive.zip -d drive/MyDrive/nlp/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/MyDrive/nlp/archive.zip\n",
            "replace drive/MyDrive/nlp/reddit_data.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygI9oRKa0hjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd8555c6-75b8-4656-a1d1-ac682bc27570"
      },
      "source": [
        "!git clone https://github.com/linanqiu/reddit-dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'reddit-dataset' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLoRhA7JPHKy",
        "outputId": "8b952b69-a30d-452f-b831-c96b473fc938"
      },
      "source": [
        "!pip install hnswlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hnswlib in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hnswlib) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQqe0jLn3fPh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c73c3da-9461-4b4f-b570-ae9e9b21b33a"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import math\n",
        "from gensim.models import FastText\n",
        "import numpy as np\n",
        "import hnswlib\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P9b5bIBUEEa"
      },
      "source": [
        "def load_big_text_df(path_to_df_dir):\n",
        "  \"\"\"\n",
        "  Делаем один большой DataFrame\n",
        "  \"\"\"\n",
        "  header=['text','id','subreddit','meta','time','author','ups','downs',\n",
        "          'authorlinkkarma','authorkarma','authorisgold']\n",
        "  df = None\n",
        "  for name in os.listdir(path_to_df_dir):\n",
        "    print(name, end='\\r')\n",
        "    if name.split('.')[-1] == 'csv':\n",
        "      current_df = pd.read_csv(os.path.join(path_to_df_dir, name), header=0, index_col=0).iloc[1:, -11:]\n",
        "      current_df.columns = header\n",
        "      if df is None:\n",
        "        df = current_df\n",
        "      else:\n",
        "        df = pd.concat([df, current_df], axis=0)\n",
        "  # return df[['text', 'subreddit', 'author']].dropna()\n",
        "  print('Loaded')\n",
        "  return df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cW0Od23iCtY",
        "outputId": "c61baeac-8b59-48bd-ae00-76bd88d64da1"
      },
      "source": [
        "text_author_topic_df = load_big_text_df('reddit-dataset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SglY5udU4KsR"
      },
      "source": [
        "user_topic_df = pd.read_csv('drive/MyDrive/nlp/reddit_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yuXCZzODOP0"
      },
      "source": [
        "Так как в датасете с github есть username и subreddict, которых нету в датасете с kaggle и наоборот, обрежим датасеты таким образом, что бы все username и subreddict пересекались"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srAWAU9k9lYf"
      },
      "source": [
        "def make_cross_sets(df_1, df_2, column_1, column_2):\n",
        "  \"\"\"\n",
        "  Получаем значения в колонках, которые есть в первом и втором df\n",
        "  \"\"\"\n",
        "  set_1 = set(df_1[column_1].values)\n",
        "  set_2 = set(df_2[column_2].values)\n",
        "  return set_1 & set_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2Vq4_y-UFyk"
      },
      "source": [
        "cross_username = make_cross_sets(df_1=text_author_topic_df, df_2=user_topic_df, column_1='author', column_2='username')\n",
        "cross_topic = make_cross_sets(df_1=text_author_topic_df, df_2=user_topic_df, column_1='subreddit', column_2='subreddit')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdQlfz-cTOLB"
      },
      "source": [
        "text_author_topic_df['match_row'] = text_author_topic_df[['author', 'subreddit']].apply(lambda x: True if (x[0] in cross_username and x[1] in cross_topic) else False, axis=1)\n",
        "user_topic_df['match_row'] = user_topic_df[['username', 'subreddit']].apply(lambda x: True if (x[0] in cross_username and x[1] in cross_topic) else False, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmP0b49UUnPy"
      },
      "source": [
        "# обрезаем датасеты\n",
        "text_author_topic_df = text_author_topic_df[text_author_topic_df['match_row'] == True]\n",
        "user_topic_df = user_topic_df[user_topic_df['match_row'] == True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4De6PeoVLvX"
      },
      "source": [
        "# для наглядности переведем время\n",
        "text_author_topic_df['time'] = text_author_topic_df['time'].apply(lambda x: datetime.datetime.fromtimestamp(x))\n",
        "user_topic_df['time'] = user_topic_df['utc'].apply(lambda x: datetime.datetime.fromtimestamp(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBCAaca7EZdw"
      },
      "source": [
        "Сразу отложим тестовые данные. Считаем, что корпус с github это тренировочные данные, а данные с kaggle - тестовые. Для того, что бы не предсказывать прошлое, уберем из последнего датасета топики, у которых время обращения раньше, чем самое позднее время, которое есть в тренировочном наборе для соответствующего пользователя "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POBoYlWxgght"
      },
      "source": [
        "def make_test_dict(text_author_topic_df, user_topic_df, cross_username):\n",
        "  \"\"\"\n",
        "  Генерируем словарь {username: [topic1, topic2 .. ]}  \n",
        "  \"\"\"\n",
        "  max_time_dict = dict(text_author_topic_df.groupby(['author', 'time'])['time'].max().index)\n",
        "  test_dict = {}\n",
        "  for username in cross_username:\n",
        "    print(username, end='\\r')\n",
        "    user_topic_df_slice = user_topic_df[user_topic_df['username'] == username] \n",
        "    test_topics = user_topic_df_slice[user_topic_df_slice['time'] > max_time_dict[username]]['subreddit'].values\n",
        "    test_dict[username] = list(set(test_topics))\n",
        "  return test_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtKJJzIw-feq"
      },
      "source": [
        "cross_username = make_cross_sets(df_1=text_author_topic_df, df_2=user_topic_df, column_1='author', column_2='username')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8EJ5k5xh_e6",
        "outputId": "d212a21b-6fa8-4799-eef7-77f9003f3355"
      },
      "source": [
        "test_dict = make_test_dict(text_author_topic_df=text_author_topic_df, user_topic_df=user_topic_df, cross_username=cross_username)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ""
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaHPUw04iDKc",
        "outputId": "be4f7c53-392e-4682-c62a-f92a86824719"
      },
      "source": [
        "# удаляем username, у которых нету топиков, удовлетворяющих нашим условиям в \n",
        "# датасете с kaggle\n",
        "drop_username = []\n",
        "for username in test_dict:\n",
        "  if len(test_dict[username]) == 0:\n",
        "    drop_username.append(username)\n",
        "    print(username)\n",
        "cross_username -= set(drop_username)\n",
        "text_author_topic_df = text_author_topic_df[text_author_topic_df['author'].apply(lambda x: x in cross_username)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "zuperpailon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5KQUKYSnOGq"
      },
      "source": [
        "# Немного предобработаем текст"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk3kvV63o45G"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LpsjeEAp-xf"
      },
      "source": [
        "special_char = ',./_=\\!?'\n",
        "def delete_special_char(text):\n",
        "  \"\"\"\n",
        "  Удаляем специальные символы\n",
        "  \"\"\"\n",
        "  for char in special_char:\n",
        "    text = text.replace(char, '')\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxEE6BtgpNC1"
      },
      "source": [
        "set_stopwords = set(stopwords.words('english'))  # чтоб искать за O(1)\n",
        "def process_text(text):\n",
        "  \"\"\"\n",
        "  Удаляем специальные символы, токинизируем и лемматизируем текст\n",
        "  :return [word1, word2, ..]\n",
        "  \"\"\"\n",
        "  text = delete_special_char(text)\n",
        "  text = word_tokenize(text)\n",
        "  return [lemmatizer.lemmatize(word) for word in text if not word in set_stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWbG0Cc4nJ5U"
      },
      "source": [
        "text_author_topic_df['words'] = text_author_topic_df['text'].apply(process_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Img4yA1K-N6I"
      },
      "source": [
        "# Некоторые пользователи пишут бессмысленные посты, состоящие только из stopwords\n",
        "text_author_topic_df = text_author_topic_df[text_author_topic_df['words'].apply(lambda x: x != [])]\n",
        "cross_username = make_cross_sets(df_1=text_author_topic_df, df_2=user_topic_df, column_1='author', column_2='username')\n",
        "cross_topic = make_cross_sets(df_1=text_author_topic_df, df_2=user_topic_df, column_1='subreddit', column_2='subreddit')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovS4Uh8LHOsY"
      },
      "source": [
        "# Учим FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgzyhCA2so7D"
      },
      "source": [
        "embedding_dim = 128\n",
        "fasttext_model = FastText(size=embedding_dim, window=3, seed=42, min_count=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luO6k4fAtcw8"
      },
      "source": [
        "fasttext_model.build_vocab(text_author_topic_df['words'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHKfhnZRvGTC"
      },
      "source": [
        "fasttext_model.train(sentences=text_author_topic_df['words'], total_examples=len(text_author_topic_df), epochs=10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rB3BbkLzHTBP"
      },
      "source": [
        "# Модель интересов пользователя и модель, описывающая топики\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK0TqQrYA2yB"
      },
      "source": [
        "class TextItemModel:\n",
        "  \"\"\"\n",
        "  В качестве item может выступать либо пользователь либо топик\n",
        "  Каждый пользователь и топик характеризуется плотным вектором, \n",
        "  который получается путем вычисления среднего значения эмбединга текстов, \n",
        "  которые относятся к конкретному item\n",
        "  Для пользователя - среднее значение эмбединга всех его текстов\n",
        "  Для топика - среднее значение эмбеддинга всех тектов на данный топик\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, fasttext_model, text_author_topic_df, group_column, words_column='words'):\n",
        "    self.model = fasttext_model\n",
        "    self.df = text_author_topic_df\n",
        "    self.group_column = group_column\n",
        "    self.words_column = words_column\n",
        "  \n",
        "  def __embedding_from_sentence(self, words):\n",
        "    \"\"\"\n",
        "    Среднее значение эмбединга текста\n",
        "    :return np.array(size=embedding_dim)\n",
        "    \"\"\"\n",
        "    return np.sum(np.array(list(map(self.model.__getitem__, words))), axis=0)\n",
        "\n",
        "  def get_item_vector(self, item):\n",
        "    \"\"\"\n",
        "    Среднее значение эмбединга для всех текстов\n",
        "    :return np.array(size=embedding_dim)\n",
        "    \"\"\"\n",
        "    return self.df[self.df[self.group_column] == item][self.words_column].apply(self.__embedding_from_sentence).mean(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxBbTokPDa3d"
      },
      "source": [
        "text_user_model = TextItemModel(fasttext_model=fasttext_model, text_author_topic_df=text_author_topic_df, group_column='author')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWmfmWMbDgBT"
      },
      "source": [
        "text_topic_model = TextItemModel(fasttext_model=fasttext_model, text_author_topic_df=text_author_topic_df, group_column='subreddit')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8UUtg_hPcvI"
      },
      "source": [
        "# Рекомендательная система"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-RHhJXVPwPB"
      },
      "source": [
        "class RecomendationSystem:\n",
        "  \"\"\"\n",
        "  Класс, позволяющий ранжировать топики для конкретных пользователей\n",
        "  При инициализации получаем вектора эмбедингов для всех топиков и строим \n",
        "  на них граф, в котором можно будет эффективно искать ближайших соседей O(log(n))\n",
        "  В камечестве меры близости считается косинусная близость - 1 - cos(v1, v2)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, text_topic_model, text_user_model):\n",
        "    topic_coords = []\n",
        "    topic_dict = {}\n",
        "    for idx, topic in enumerate(cross_topic):\n",
        "      topic_coords.append(text_topic_model.get_item_vector(topic))\n",
        "      topic_dict[idx] = topic\n",
        "    graph = hnswlib.Index(space='cosine', dim=embedding_dim)\n",
        "    graph.init_index(len(cross_topic), M = 16, ef_construction = 200, random_seed = 100)\n",
        "    graph.add_items(np.array(topic_coords), range(len(topic_coords)))\n",
        "    self.topic_dict = topic_dict\n",
        "    self.graph = graph\n",
        "    self.text_user_model = text_user_model\n",
        "\n",
        "  def get_topics(self, username, k=10):\n",
        "    \"\"\"\n",
        "    k - число наиболее близких топиков, которые необходимо вывести\n",
        "    :return np.array([topic_1, topic_2, ..]), np.array([1 - cos(user, topic_1), 1 - cos(user, topic_2), ..])\n",
        "    \"\"\"\n",
        "    user_embedding = self.text_user_model.get_item_vector(username)\n",
        "    neighbors, similarity = self.graph.knn_query(user_embedding, k=10)\n",
        "    topics = np.array(list(map(self.topic_dict.__getitem__, neighbors[0])))\n",
        "    return topics, similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CxsuwXmRtjw",
        "outputId": "591e97dc-e3c2-4c47-c989-1b8d558c40c9"
      },
      "source": [
        "recomend_sys = RecomendationSystem(text_topic_model=text_topic_model, text_user_model=text_user_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyaR4TQS47D4",
        "outputId": "2dcc2a57-5f31-4064-8e7a-f0fd4504861f"
      },
      "source": [
        "recomend_sys.get_topics(username=list(cross_username)[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['thewalkingdead', 'politics', 'funny', 'skyrim', 'startrek',\n",
              "        'guns', 'food', 'explainlikeimfive', 'motorcycles', 'comicbooks'],\n",
              "       dtype='<U17'),\n",
              " array([[0.38961184, 0.403373  , 0.40477908, 0.40536213, 0.40542394,\n",
              "         0.40674466, 0.41098273, 0.4191075 , 0.4209404 , 0.4217881 ]],\n",
              "       dtype=float32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhIwlwBU5B4u"
      },
      "source": [
        "# Валидация\n",
        "Допустим, что у нас есть возможность показать пользователю 10 различных топиков, тогда в качестве метрики будем использовать число топиков выведенных топиков (из тестового датасета), на которые в дальнейшем пользователь написал пост"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH9OzA80aIE2",
        "outputId": "6e4f4953-0d6a-4472-e31b-434ed5e66197"
      },
      "source": [
        "predict_dict = {}\n",
        "for user in cross_username:\n",
        "  topics, _ = recomend_sys.get_topics(user)\n",
        "  predict_dict[user] = topics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TjxIH_QzNgV"
      },
      "source": [
        "cross_topics_validation = []\n",
        "for user in predict_dict:\n",
        "  cross_topics_validation.append(len(set(predict_dict[user]) & set(test_dict[user])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzsEuqy9-jSh",
        "outputId": "42528332-fa91-4727-8b06-9b43e19c1002"
      },
      "source": [
        "np.mean(cross_topics_validation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2454819277108435"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlCXVYHx6Z5O"
      },
      "source": [
        "Какое бы значение имела данная метрика, если бы мы случайно угадывали топики?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cldIq2nVxc_F"
      },
      "source": [
        "def combinations(n, k):\n",
        "  \"\"\"\n",
        "  Число сочетаний\n",
        "  \"\"\"\n",
        "  return math.factorial(n) / (math.factorial(k) * math.factorial(n - k))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ4KLrv6jbGj"
      },
      "source": [
        "def p(n, m, k, good_topics):\n",
        "  \"\"\"\n",
        "  Вероятность того, что число правильно выведенных топиков равно good_topics\n",
        "  n - всего топиков, m - число топиков, на которые пользователь написал пост\n",
        "  k = 10 - число выводимых топиков\n",
        "  \"\"\"\n",
        "  p = 1\n",
        "  for i in range(good_topics):\n",
        "    p *= (m - i) / (n - i)\n",
        "    if i > k:\n",
        "      return p\n",
        "  for i in range(0, k - good_topics):\n",
        "    p *= (n - m - i)/(n - i - good_topics)\n",
        "  if k < good_topics:\n",
        "    good_topics = k\n",
        "  return combinations(k, good_topics) * p "
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmxcM533nT-6"
      },
      "source": [
        "def expectation_good_topics(n, m, k):\n",
        "  \"\"\"\n",
        "  Математическое ожидание числа правильно показанных топиков\n",
        "  \"\"\"\n",
        "  expectation = 0\n",
        "  for i in range(1, m + 1):\n",
        "    expectation += i * p(n, m, k, i)\n",
        "  return expectation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEnyWnn3qjmB",
        "outputId": "9642113a-3380-4582-b96e-bba56def5ca6"
      },
      "source": [
        "# проверим все ли хорошо с формулами\n",
        "all_topics = list(range(40))\n",
        "good_topics = list(range(5))\n",
        "match = []\n",
        "for i in range(10000):\n",
        "  topics = np.random.choice(all_topics, size = 10, replace=False)\n",
        "  match.append(len(set(topics) & set(good_topics)))\n",
        "print(f'Действительное значение: {np.mean(match)}')\n",
        "print(f'Теоретическое: {expectation_good_topics(40, 5, 10)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Действительное значение: 1.2555\n",
            "Теоретическое: 1.2499999999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SguLrXh4osZe"
      },
      "source": [
        "for user in cross_username:\n",
        "  topics = np.random.choice(list(cross_topic), size=10, replace=False)\n",
        "  predict_dict[user] = topics"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mf5pgo3qMpT"
      },
      "source": [
        "cross_topics_validation_random = []\n",
        "for user in predict_dict:\n",
        "  cross_topics_validation_random.append(len(set(predict_dict[user]) & set(test_dict[user])))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAzoKu3xqNEk",
        "outputId": "25eb17eb-a5f7-4321-8b81-69e37e4d8473"
      },
      "source": [
        "np.mean(cross_topics_validation_random)  # брут форс"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7846385542168675"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhvR4iGNEw5c"
      },
      "source": [
        "total_topics = len(cross_topic)\n",
        "mean_good_topics = []\n",
        "k = 10\n",
        "for user in cross_username:\n",
        "  m = len(test_dict[user])\n",
        "  mean_good_topics.append(expectation_good_topics(total_topics, m, k))"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_bzwFHrFdyd",
        "outputId": "f98d9533-4552-4f80-a57e-f0580fc3d9c4"
      },
      "source": [
        "np.mean(mean_good_topics)  # по формуле"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7987106085153497"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zzNyT3xrC7c",
        "outputId": "37d5e404-9284-4c18-b751-c33fb92b8589"
      },
      "source": [
        "np.mean(cross_topics_validation)  # модель"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2454819277108435"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K0TVcnM7iv0"
      },
      "source": [
        "Модель предсказывает лучше случайного выбора, но не на много"
      ]
    }
  ]
}